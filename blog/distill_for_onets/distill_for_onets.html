<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog</title>
    <link rel="stylesheet" id="ns-minimal-google-font-css" href="https://fonts.googleapis.com/css?family=Nunito+Sans%3A300%2C400%2C400i%2C700%2C700i&amp;subset=latin%2Clatin-ext" type="text/css" media="all">
    <link rel="stylesheet" href="../../css/main.css">
    <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body class="page-blog">
<div class="content-wrapper">
    <header>
        <p style="text-align: center;"><a href="index.html">‚Üê Back to Home</a></p>
        <h3 style="text-align: center;">Measuring things in machine learning</h3>
        <h5 style="text-align: center;">Nov. 9, 2025</h5>
        
    </header>


<article>

$$\DeclareMathOperator*{\argmin}{argmin}$$

<p>
A little over a year ago, I started working on applying <a href="https://github.com/Guang000/Awesome-Dataset-Distillation">dataset distillation</a> to <a href="https://en.wikipedia.org/wiki/Neural_operators">neural operators</a> (or, more accurately, to the data that trains neural operators). The main goal was to reduce the amount of data needed to train a performant network. But a good compression algorithm comes with the perk that you learn lots of interesting things about what the model's paying attention to, in the same way that you learn things about yourself and your priorities when you're cleaning out your apartment.
</p>

<p>
The issue was that while distillation really shone in classification settings (a good distilled dataset can nearly solve MNIST by now), it'd been under-applied to regression problems of the kind we see in science, except in certain limited domains where we could recast classification problems into the kernel regression setting to take advantage of interesting properties there. But neural networks view operator learning as a kind of function-to-function regression problem, essentially performing regression from some set of coefficients and functional conditions into the space of functional solutions to a given class of differential equations.
</p>

<p>
I thought (and still think) that applying dataset distillation to neural operator data would be a narrow yet worthwhile exercise, not least because it gives us a sense of how scientific data is represented to NNs.
But this hang-up led me to strip down the problem and study distillation on a few <a href="https://openreview.net/forum?id=bM4MbgGnpx">very simple regression settings</a>, with the intention of returning to the neural operator setting later. Then later came around, so here we are.
</p>

<header>
    <h4 style="text-align: center;">Neural operators</h4>
<p>
Neural operators, and neural solvers more generally, first need to adapt traditional numerical solvers into a clean input-output structure. To see this in action, we start with the constant-diffusivity form of the time-dynamic, one-dimensional heat equation, complete with a sinusoidal initial condition and Dirichlet boundary conditions: 
\[
\begin{cases} 
\text{PDE: } \frac{\partial u}{\partial t} = \alpha \cdot \frac{\partial^2 u}{\partial x^2} \\
\\
\text{IC: } u(x, 0) = \sin(\pi x) \\
\\
\text{BCs: } u(0, t) = u(1, t) = 0
\end{cases}
\]

<p>The initial and boundary conditions form a "heat cup" on the graph of temperature over time:</p>
<img src="images/heat_conditions_only.png" alt="Heat cup boundary conditions illustration" style="display: block; margin: 0 auto; max-width: 400px;">
<p>Using the form of the PDE and a given constant \( \alpha \), the finite difference method is sufficient to fill in the rest of the cup before the frostbite sets in:</p>
<img src="images/heat.png" alt="Heat cup boundary conditions illustration" style="display: block; margin: 0 auto; max-width: 400px;">

<p>Traditionally, we view the filling-in process ‚Äì‚Äì in this case, the finite-difference method ‚Äì‚Äì as a sequence of discrete linear functions.
That is, given fixed mesh grains \(dt\) and \(dx\), we put \( r = \alpha \frac{dt}{dx^2} \), and set
\[ u(x_i, t_{j+1}) = u(x_i, t_j) + r \cdot u(x_{i+1}, t_j) - 2 \cdot u(x_i, t_j) + u(x_{i-1},t_j) \]
for \( i = 1, ..., n_x-1 \) and \( j = 0, ..., n_t \).
This gives us \( (n_x - 2)(n_t - 1) \) total equations that we run in sequence.
</p>


<p>To make this palatable for neural networks, we shift the perspective a bit by representing the entire process as a function \(F\) that takes an empty heat cup and fills it up according to a given \(\alpha\). Assume that we're OK (for now) keeping a constant mesh on the domain \( [0,1]^2 \). Then our function should do something like this:</p>
<img src="images/function.png" alt="Heat cup function illustration" style="display: block; margin: 0 auto; max-width: 400px;">
We can compare this to the functional representations of other kinds of models:
<ul>
    <li>
        Image-to-text classification: \(F(\)
        <img src="images/cat.png" alt="Cat" style="vertical-align: middle; max-width: 40px; margin: 0 4px;">
        \() = \; \) "cat".
    </li>
    <br>
    <li>
        Text-to-text:
        \(F(\) "Who you got for the '26 CFB championship?"\() = \) "The horns baby ü§ò"
    </li>
    <br>
    <li>
        Text-to-image: \(F(\) "Abe Lincoln dunking a flaming basketball"\()=\)
         <img src="images/abe.png" alt="Abe" style="vertical-align: middle; max-width: 40px; margin: 0 4px;">
    </li>
</ul>
I think it bears similarity in form, though not (necessarily) in method, to the process of denoising or filling in an incomplete photo:

<p>Ours is a function-function mapping, which we call an <b>operator</b>.
Rewriting our conditions as \( u(x, 0) = u_0(x) \), \(\; u(0, t) = u_L(t)\), \(\;\)and \( u(1, t) = u_R(t) \), we can formalize our operator as 
\[F_{\alpha}(u_0, u_L, u_R) = u(x, t),\]
where \( F_{\alpha} \) is the operator parameterized by diffusivity \( \alpha \) that maps from the initial and boundary conditions (which in our example are real-valued functions \(\mathbb{R} \to \mathbb{R}\)) to the solution function \( u(x, t) \) (also a real-valued function \(\mathbb{R}^2 \to \mathbb{R}\)). The method we use to find this operator is a growing subfield of machine learning.</p>

<p>There are some important distinctions between our operator and the more consumer-facing kind we're used to. The biggest is that most of these models have a desirable element of creative randomness. Ask your favorite language model to write a poem three times and you'll get three different poems, each uniquely bad. This randomness is mediated in the sampling procedures we apply to the probability distributions derived from the logits, with temperature controlling how peaked or flat those distributions are before sampling. Higher temperatures cause the model to make more exotic word choices, which is a desirable property in some cases. Generative vision models have even more randomness modes ‚Äì‚Äì in diffusion models, we randomly initialize a tensor, randomly add noise, and (often, but not always) randomly remove noise. The result is that asking the same prompt almost never gives the same result.</p>

<p>
We don't want the same element of creative diversity in scientific modeling. When randomness appears, it's typically either during training (for optimization) or to explicitly model uncertainty and variability in the underlying system, rather than to generate novel creative outputs.
</p>

<header>
<h4 style="text-align: center;">The world's briefest introduction to data distillation</h4>

<p>
Putting aside the operator nonsense for a second, we can define data distillation in its most general sense as an algorithm that optimizes a small dataset toward the task of training a performant model. Given a model architecture \( g( \cdot \:; \theta) \) with a loss \( \mathcal{L} \), and a dataset \( X \) of \( n \) samples with labels \( Y \), we're looking for a dataset \((\tilde{X}, \tilde{Y})\) with \( m \) samples, \( m << n \), where
\[ \tilde{X}^* \in \argmin_\tilde{X} \; \mathcal{L}[ \; g(X; \theta_{\tilde{X}}); Y \;] \quad \text{ s.t. } \quad \theta_{\tilde{X}} \in \argmin_{\theta} \; \mathcal{L}[\; g(\tilde{X}; \theta), \; Y].\]
</p>
<p>
I'm glossing over the fact that, for optimization purposes, the loss \( \mathcal{L} \) is allowed to be slightly different between the two terms, but the motivation is the same. Also, it's worth underscoring that for the algorithm to make sense, it needs a particular model architecture to optimize toward. There is no "general" distilled dataset that works for any model architecture, though there are some promising directions in that regard.
</p>
<p>
Barring a closed-form minimizer, the simplest route is to perform both these optimizations through gradient descent. Here, randomly initialize \(\tilde{\mathbf{x}} :=(\tilde{X}, \tilde{Y})\) with which we train an ephemeral set of weights \(\theta\) for some number of steps. Then we take this "trained" model and evaluate it on real data, obtaining a grade of its performance. Finally, we compute the derivative of this loss <i>with respect to the data</i>, updating the data itself so that in the next loop, it trains a model that achieves better performance when evaluated on the real data. We run this until "converged", by which we mean that the mini-model trained on the final set of distilled dataset does a somewhat decent job evaluating the real data.
</p>

<p>
The process of optimizing the data looks a lot like "filling in" a set of data from its random initialization, which can look like diffusion if you squint hard enough.
</p>

<p>
Image classification is definitely the canonical test case for distillation: the <a href="https://www.tongzhouwang.info/dataset_distillation/">original paper</a> operated on MNIST and CIFAR, and most theoretical advances have defaulted to the image classification setting. These make for good visualizations, like this headline visual from <a href="https://arxiv.org/pdf/2203.11932">Cazanavette et al.</a>:
<br>
<img src="images/cazanavette_images.png" alt="Cazanavette distilled images" style="display: block; margin: 0 auto; max-width: 600px;">
</p>
There's no reason we can't apply the same principles to other kinds of problems, and people have tried to distill 
<a href="https://arxiv.org/abs/2310.09202">all</a>
<a href="https://dl.acm.org/doi/10.1145/3604915.3608769">sorts</a> 
<a href="https://arxiv.org/abs/2104.08448">of</a>
<a href="https://arxiv.org/abs/2412.00111">data</a>
<a href="https://arxiv.org/abs/2406.02963">types</a>. I am interested in trying to distill scientific data, which to my knowledge hasn't been done before.

<header>
<h4 style="text-align: center;">Distilling for regression-like problems</h4>
<p>In a <a href="https://openreview.net/pdf?id=bM4MbgGnpx">paper</a> we wrote a few months ago, we showed that how fast a distillation algorithm converges (and indeed, whether it converges at all) is mediated by how convex your objective function is. 
Here, for input-lable pairs \( (X,Y) = \mathbf{x} \), we used 
\[\mathcal{L} = ||\tilde{\theta} X - Y||^2 + \lambda(||\tilde{X}||_F^2 + ||\tilde{Y}||_F^2), \]
where \( ||\cdot||_F \) is the Frobenius norm and \( \tilde{\theta} \) represents the mini-model trained by that loop's distilled data \(\tilde{\mathbf{x}} = (\tilde{X}, \tilde{Y}).\) The weird-looking regularization term \( \lambda ||\tilde{X}||_F^2 \) ensures that when we make our gradient descent steps, we're don't tread too far from the ball around the origin where the original data lives.
</p>
<p>
While convergence is guaranteed for a strongly convex objective (whose Hessian is positive-definite), we're're forced to add larger and larger regularization terms when our Hessian starts displaying negative eigenvalues. Cueing from traditional optimization, our regularization term \( \lambda \) must be larger than the absolute value of the most negative eigenvalue \( M \) to guarantee linear convergence that depends on \( \lambda - M \).
</p>
<p>
This is important because operator networks tend to use an \( L^2 \) mean-squared error loss, which can show up in several versions:
<ul>
    <li>
        Supervised samples:
        \(\; \mathcal{L} = ||u_{\text{pred}}(x,t) - u_{\text{true}}(x,t)||^2 \) (divide by \( ||u_{\text{true}}(x,t)||^2 \) for relative error).
    </li>
    <li>
        Physics-informed (PDE residual):
        \(\; \mathcal{L} = ||\frac{\partial u}{\partial t} - \alpha \frac{\partial^2 u}{\partial x^2}  ||^2 \)
    </li>
    <li>
        \( H^1 \) norm:
        \(\; \mathcal{L} = ||u_{\text{pred}} - u_{\text{true}}||^2 + ||\nabla u_{\text{pred}} - \nabla u_{\text{true}}||^2 \) (accounts for smoothness and energy constraints).
    </li>
</ul>

[WIP]

</p>
    <footer style="margin-top: 40px;">
        <p style="text-align: center;">&copy; All rights reserved.</p>
    </footer>
</body>
</html>